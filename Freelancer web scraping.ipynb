{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n",
      "Page Loading wait....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib \n",
    "import os\n",
    "from time import sleep\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "jobtitle = []\n",
    "\n",
    "for page in range(0,3000,50):\n",
    "    URL = \"https://www.freelancer.com/job/?ngsw-bypass=&w=f\".format(page)\n",
    "    print ('Page Loading wait....')\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    job_html = soup.find_all('a',{'PageJob-category-link':'title'})\n",
    "      \n",
    "    for job in job_html:\n",
    "        try:\n",
    "            jobtitle.append(job.text)\n",
    "        except:\n",
    "            jobtitle.append('nothing_found')\n",
    "                                                         \n",
    "sleep(max([1,np.random.normal(4,2)]))            \n",
    "            \n",
    "d1 = {'title':jobtitle}\n",
    "\n",
    "df1 = pd.DataFrame(data=d1)\n",
    "df1      \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyQt5.QtWebEngineWidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-14fd9b03403b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPyQt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQtWidgets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQApplication\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPyQt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQtCore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQEventLoop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPyQt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQtWebEngineWidgets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQWebEnginePage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyQt5.QtWebEngineWidgets'"
     ]
    }
   ],
   "source": [
    "   import sys\n",
    "from PyQt5.QtWidgets import QApplication\n",
    "from PyQt5.QtCore import QUrl, QEventLoop\n",
    "from PyQt5.QtWebEngineWidgets import QWebEnginePage\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "\n",
    "class Client(QWebEnginePage):\n",
    "    def __init__(self, url):\n",
    "        self.app = QApplication(sys.argv)\n",
    "        QWebEnginePage.__init__(self)\n",
    "        self.html = ''\n",
    "        self.loadFinished.connect(self._on_load_finished)\n",
    "        self.load(QUrl(url))\n",
    "        self.app.exec_()\n",
    "\n",
    "    def _on_load_finished(self):\n",
    "        self.html = self.toHtml(self.Callable)\n",
    "        print('Load finished')\n",
    "\n",
    "    def Callable(self, html_str):\n",
    "        self.html = html_str\n",
    "        self.app.quit()\n",
    "\n",
    "\n",
    "# MAIN\n",
    "\n",
    "page = Client(\n",
    "    'https://www.air.irctc.co.in/onewaytrip?type=O&origin=BOM&originCity=Mumbai&originCountry=IN&destination=BLR&destinationCity=Bengaluru&destinationCountry=IN&flight_depart_date=2020-06-30&ADT=1&CHD=0&INF=0&class=PremiumEconomy&airlines=UK&ltc=0')\n",
    "\n",
    "soup = BeautifulSoup(page.html, 'lxml')\n",
    "\n",
    "\n",
    "airline = soup.find_all('div', class_='right_Airline_no')\n",
    "price = soup.find_all('strong', class_=\"red-text\")\n",
    "dep_time = soup.find_all('div', class_='SearchData_List_in SearchData_Departure font-14')\n",
    "arr_time = soup.find_all('div', class_=\"SearchData_List_in SearchData_Arrival font-14\")\n",
    "duration = soup.find_all('div', class_=\"SearchData_List_in SearchData_Duration font-14\")\n",
    "status = soup.find_all('div', class_=\"SearchData_List_in SearchData_Price\")\n",
    "\n",
    "csv_file = open('MUM-BEN1-PE.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['flight_no', 'airline', 'dep_time', 'duration', 'stop', 'arr_time', 'price1'])\n",
    "\n",
    "number_l = list()\n",
    "name_l = list()\n",
    "for no in airline:\n",
    "    number = no.find_all('span')[1]\n",
    "    name = no.find_all('span')[0]\n",
    "    number_l.append(number.get_text())\n",
    "    name_l.append(name.get_text())\n",
    "\n",
    "dep_time_l = list()\n",
    "for dtime in dep_time:\n",
    "    time = dtime.find('strong')\n",
    "    dep_time_l.append(time.get_text())\n",
    "    # print(time.get_text())\n",
    "\n",
    "price_l = list()\n",
    "for p in price:\n",
    "    l_price = list(p.get_text())\n",
    "    v = int(''.join(l_price[2:len(l_price)]))\n",
    "    # print(v)\n",
    "    price_l.append(v)\n",
    "\n",
    "arr_time_l = list()\n",
    "for atime in arr_time:\n",
    "    time = atime.find('strong')\n",
    "    # print(time.get_text(),'--',stop.get_text())\n",
    "    arr_time_l.append(time.get_text())\n",
    "\n",
    "duration_l = list()\n",
    "stop_l = list()\n",
    "for d in duration:\n",
    "    dur = d.find('strong')\n",
    "    stop = d.find('span')\n",
    "    # print(dur.get_text(),'--',stop.get_text())\n",
    "    duration_l.append(dur.get_text())\n",
    "    stop_l.append(stop.get_text())\n",
    "\n",
    "for i in range(0, len(price_l)):\n",
    "    csv_writer.writerow([number_l[i], name_l[i], dep_time_l[i], duration_l[i], stop_l[i], arr_time_l[i], price_l[i]])\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "'''    \n",
    "count = 0\n",
    "for r in status:\n",
    "    st = r.find(class_=\"font-12\")\n",
    "    if st.get_text() == \"(Partially Refundable)\":\n",
    "        count=count+1\n",
    "    if st.get_text() == \"(Non Refundable)\":\n",
    "        count=count+1\n",
    "print(count)\n",
    "'''\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\y shruthi mol\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import QApplication, QWidget\n",
    "from PyQt5.QtGui import QIcon\n",
    "\n",
    "class App(QWidget):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title = 'PyQt5 simple window - pythonspot.com'\n",
    "        self.left = 10\n",
    "        self.top = 10\n",
    "        self.width = 640\n",
    "        self.height = 480\n",
    "        self.initUI()\n",
    "        \n",
    "    def initUI(self):\n",
    "        self.setWindowTitle(self.title)\n",
    "        self.setGeometry(self.left, self.top, self.width, self.height)\n",
    "        self.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app = QApplication(sys.argv)\n",
    "    ex = App()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Scrapy Library\n",
    "import scrapy\n",
    " \n",
    "# Creating a new class to implement Spide\n",
    "class AmazonReviewsSpider(scrapy.Spider):\n",
    " \n",
    "    # Spider name\n",
    "    name = 'amazon_reviews'\n",
    " \n",
    "    # Domain names to scrape\n",
    "    allowed_domains = ['amazon.in']\n",
    " \n",
    "    # Base URL for the MacBook air reviews\n",
    "    myBaseUrl = \"https://www.amazon.in/Apple-MacBook-Air-13-3-inch-MQD32HN/product-reviews/B073Q5R6VR/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;amp;reviewerType=all_reviews&amp;amp;pageNumber=\"\n",
    "    start_urls=[]\n",
    " \n",
    "    # Creating list of urls to be scraped by appending page number a the end of base url\n",
    "    for i in range(1,121):\n",
    "        start_urls.append(myBaseUrl+str(i))\n",
    " \n",
    "    # Defining a Scrapy parser\n",
    "    def parse(self, response):\n",
    "            data = response.css('#cm_cr-review_list')\n",
    " \n",
    "            # Collecting product star ratings\n",
    "            star_rating = data.css('.review-rating')\n",
    " \n",
    "            # Collecting user reviews\n",
    "            comments = data.css('.review-text')\n",
    "            count = 0\n",
    " \n",
    "            # Combining the results\n",
    "            for review in star_rating:\n",
    "                yield{'stars': ''.join(review.xpath('.//text()').extract()),\n",
    "                      'comment': ''.join(comments[count].xpath(\".//text()\").extract())\n",
    "                     }\n",
    "                count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
